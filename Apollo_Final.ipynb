{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install pdfplumber spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Mannuly Specify the content table page number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text manuplating "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Get a Dataframe of Item with the range of corresponding pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# By default, the function extracts items from page 2 of the PDF\n",
    "def item_pages_df(pdf_path, page_num=2):\n",
    "    # Function to extract item pages from a PDF file\n",
    "    # pdf_path: Path to the PDF file\n",
    "    # Returns a DataFrame with item titles and their corresponding page numbers\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        page = pdf.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "    sentences = re.split(r'\\n', text)\n",
    "    items_dict = {}\n",
    "    pattern = re.compile(r'(Item \\d+[A-Z]?\\..*?)\\s+(\\d+)$')\n",
    "    for sentence in sentences:\n",
    "        match = pattern.search(sentence)\n",
    "        if match:\n",
    "            title = match.group(1).strip()\n",
    "            page = int(match.group(2))\n",
    "            items_dict[title] = page\n",
    "    return pd.DataFrame(items_dict.items(), columns=[\"Title\", \"Page Number\"]), items_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Extract sentences from the PDF based on the Item. \n",
    "    Return a dataframe of sentences in th Item\n",
    "    Example: extract_item_specific(\"Item 1A. Business\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def extract_item_specific(selected_item, items_dict, pdf_file):\n",
    "\n",
    "\n",
    "    # Check if the selected item exists\n",
    "    if selected_item not in items_dict:\n",
    "        print(f\"Error: '{selected_item}' not found in items_dict. Please check the spelling.\")\n",
    "        exit()\n",
    "\n",
    "    # Get the start and end page for the selected item\n",
    "    sorted_items = sorted(items_dict.items(), key=lambda x: x[1])  # Sort items by page numbers\n",
    "    start_page = items_dict[selected_item]\n",
    "    end_page = None\n",
    "\n",
    "    # Find the next item's page to determine the range\n",
    "    for i in range(len(sorted_items)):\n",
    "        if sorted_items[i][0] == selected_item:\n",
    "            end_page = sorted_items[i + 1][1] - 1 if i + 1 < len(sorted_items) else None\n",
    "            break\n",
    "\n",
    "    # Dictionary to store extracted text for the selected item\n",
    "    extracted_text = \"\"\n",
    "\n",
    "    # Open the PDF and extract only the required section\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        for page_num in range(start_page - 1, (end_page or len(pdf.pages))):  # Zero-based index\n",
    "            try:\n",
    "                page_text = pdf.pages[page_num].extract_text()\n",
    "                if page_text:\n",
    "                    extracted_text += \"\\n\" + page_text.strip()\n",
    "            except IndexError:\n",
    "                print(f\"Warning: Page {page_num + 1} out of range.\")\n",
    "                break\n",
    "\n",
    "    # Clean and tokenize the extracted text\n",
    "    clean_text = re.sub(r'\\s+', ' ', extracted_text).strip()  # Remove extra spaces/newlines\n",
    "    clean_text = re.sub(r'[^A-Za-z0-9,.?! ]+', '', clean_text)  # Remove special characters\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = sent_tokenize(clean_text)\n",
    "\n",
    "    # Store as a DataFrame\n",
    "    df_selected_item = pd.DataFrame(sentences, columns=[\"Sentences\"])\n",
    "    return df_selected_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Extracts key words related to a given target word (e.g., 'risk') using dependency parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ✅ Step 1: Preprocess Text (KEEP Stop Words)\n",
    "def preprocess_text(df):\n",
    "    \"\"\"Process text by tokenizing, lemmatizing, and keeping all stopwords.\"\"\"\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in df[\"Sentences\"]:\n",
    "        doc = nlp(sentence)\n",
    "        tokens = [token.lemma_ for token in doc if token.is_alpha]  # Keep all stop words\n",
    "        processed_sentences.append(\" \".join(tokens))\n",
    "    \n",
    "    return pd.DataFrame(processed_sentences, columns=[\"Processed Sentences\"])\n",
    "\n",
    "# ✅ Step 2: Extract Context Words (EXCLUDE Stop Words in Counting)\n",
    "def extract_context_words(df, target_word, num_display=20, column_name=\"Processed Sentences\"):\n",
    "    \"\"\"Extracts key words related to a given target word (e.g., 'risk') using dependency parsing.\"\"\"\n",
    "    related_words = []\n",
    "    stop_words = nlp.Defaults.stop_words  # Get default stop words from spaCy\n",
    "\n",
    "    # Ensure DataFrame is sorted consistently\n",
    "    df = df.sort_values(by=column_name, ignore_index=True)\n",
    "\n",
    "    for sentence in df[column_name]:\n",
    "        doc = nlp(sentence)\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.text.lower() == target_word.lower():\n",
    "                # Collect adjectives modifying \"risk\" (e.g., \"high risk\", \"financial risk\")\n",
    "                for child in token.children:\n",
    "                    if child.dep_ in [\"amod\", \"compound\", \"nsubj\", \"prep\"] and child.text.lower() not in stop_words:\n",
    "                        related_words.append(child.text.lower())\n",
    "\n",
    "                # Collect nouns associated with \"risk\" (e.g., \"market risk\", \"liquidity risk\")\n",
    "                if token.head.pos_ in [\"NOUN\", \"PROPN\"] and token.head.text.lower() not in stop_words:\n",
    "                    related_words.append(token.head.text.lower())\n",
    "\n",
    "                # Find adjectives appearing **later in the sentence** (e.g., \"risk is inefficient\")\n",
    "                for ancestor in token.ancestors:\n",
    "                    if ancestor.dep_ in [\"acomp\"] and ancestor.text.lower() not in stop_words:\n",
    "                        related_words.append(ancestor.text.lower())\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    word_counts = Counter(related_words).most_common()\n",
    "    df_words = pd.DataFrame(word_counts, columns=[\"Keyword\", \"Frequency\"])\n",
    "    \n",
    "    return df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis and polarize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_word_sentiment(df):\n",
    "    \"\"\"Computes sentiment polarity (-1 to 1) for extracted words using VADER.\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df[\"Sentiment\"] = df[\"Keyword\"].apply(lambda word: analyzer.polarity_scores(word)['compound'])\n",
    "\n",
    "    # Calculate weighted sentiment\n",
    "    df[\"Weighted Sentiment\"] = df[\"Sentiment\"] * df[\"Frequency\"]\n",
    "    average_sentiment = df[\"Weighted Sentiment\"].sum() / df[\"Frequency\"].sum()\n",
    "\n",
    "    # Exclude neutral words\n",
    "    df_non_neutral = df[df[\"Sentiment\"] != 0.0]\n",
    "    non_neutral_sentiment = df_non_neutral[\"Weighted Sentiment\"].sum() / df_non_neutral[\"Frequency\"].sum() if not df_non_neutral.empty else 0.0\n",
    "\n",
    "    print(f\"Weighted Sentiment Score (Including Neutral Words): {average_sentiment}\")\n",
    "    print(f\"Weighted Sentiment Score (Excluding Neutral Words): {non_neutral_sentiment}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepocessing for 2023 10-k Starting here\n",
    "# 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Page Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Item 1. Business</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item 1A. Risk Factors</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Item 1B. Unresolved Staff Comments</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Item 1C. Cybersecurity</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Item 2. Properties</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Item 3. Legal Proceedings</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Item 4. Mine Safety Disclosures</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Item 5. Market for Registrant's Common Equity,...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Item 6. [Reserved]</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Item 7. Management's Discussion and Analysis o...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Item 7A. Quantitative and Qualitative Disclosu...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Item 8. Financial Statements and Supplementary...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Item 8A. Unaudited Supplemental Presentation o...</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Item 9. Changes in and Disagreements with Acco...</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Item 9A. Controls and Procedures</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Item 9B. Other Information</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Item 9C. Disclosure Regarding Foreign Jurisdic...</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Item 10. Directors, Executive Officers and Cor...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Item 11. Executive Compensation</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Item 12. Security Ownership of Certain Benefic...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Item 13. Certain Relationships and Related Tra...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Item 14. Principal Accountant Fees and Services</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Item 15. Exhibits, Financial Statement Schedules</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Item 16. Form 10-K Summary</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  Page Number\n",
       "0                                    Item 1. Business           13\n",
       "1                               Item 1A. Risk Factors           33\n",
       "2                  Item 1B. Unresolved Staff Comments           59\n",
       "3                              Item 1C. Cybersecurity           59\n",
       "4                                  Item 2. Properties           62\n",
       "5                           Item 3. Legal Proceedings           62\n",
       "6                     Item 4. Mine Safety Disclosures           62\n",
       "7   Item 5. Market for Registrant's Common Equity,...           63\n",
       "8                                  Item 6. [Reserved]           65\n",
       "9   Item 7. Management's Discussion and Analysis o...           66\n",
       "10  Item 7A. Quantitative and Qualitative Disclosu...          122\n",
       "11  Item 8. Financial Statements and Supplementary...          131\n",
       "12  Item 8A. Unaudited Supplemental Presentation o...          252\n",
       "13  Item 9. Changes in and Disagreements with Acco...          256\n",
       "14                   Item 9A. Controls and Procedures          256\n",
       "15                         Item 9B. Other Information          257\n",
       "16  Item 9C. Disclosure Regarding Foreign Jurisdic...          257\n",
       "17  Item 10. Directors, Executive Officers and Cor...          258\n",
       "18                    Item 11. Executive Compensation          258\n",
       "19  Item 12. Security Ownership of Certain Benefic...          258\n",
       "20  Item 13. Certain Relationships and Related Tra...          258\n",
       "21    Item 14. Principal Accountant Fees and Services          258\n",
       "22   Item 15. Exhibits, Financial Statement Schedules          259\n",
       "23                         Item 16. Form 10-K Summary          274"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the pdf\n",
    "pdf_file = \"Apollo-23-10-k.pdf\"  # Replace with actual file path\n",
    "\n",
    "# Items pages dataframe (input the page number mannuly)\n",
    "pages_df = item_pages_df(pdf_path=pdf_file, page_num=2)[0]\n",
    "pages_dict = item_pages_df(pdf_path=pdf_file, page_num=2)[1]\n",
    "\n",
    "pages_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.How it has changed from 2023-2024 (Risk Factors more or less and to what degree) \n",
    "i. Risk Profiling by segmenting key words from 1A from both 2023 and 2024\n",
    "\n",
    "    # key word: “risk”\n",
    "    # range: Item 1A. Risk Factors\n",
    "    # year: 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific Item range of interest\n",
    "df_item = extract_item_specific(\"Item 1A. Risk Factors\", items_dict=pages_dict, pdf_file=pdf_file)\n",
    "# Preprocess the text(tokenize, lemmatize)\n",
    "df_item_processed = preprocess_text(df_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract context words\n",
    "# Extract context words related to \"risk\"\n",
    "# Count the frequency of each word and display the top 30 words\n",
    "context_words = extract_context_words(df_item_processed, \"risk\", num_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sentiment Score (Including Neutral Words): 0.04088407079646017\n",
      "Weighted Sentiment Score (Excluding Neutral Words): 0.16499642857142857\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Weighted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>credit</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>2.6726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>associate</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>concentration</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>additional</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>significant</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>0.8092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>litigation</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.2023</td>\n",
       "      <td>-0.6069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>regulatory</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>expose</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>-0.4593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>liquidity</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>exposure</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>certain</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.5464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>unlimited</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>great</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6249</td>\n",
       "      <td>1.2498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>numerous</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>transition</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>counterparty</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>factors</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>factor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>enforcement</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>compliance</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>special</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>difficulty</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>-0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>climaterelate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>changerelate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>condition</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>security</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>inherent</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>increase</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>potential</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>pose</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>interested</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>0.4019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>appetite</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>management</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>seek</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>default</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>inaccuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>price</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>rate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>market</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>entail</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>foreclosure</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>-0.1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>delinquency</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>type</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>macroeconomic</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>stem</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>single</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>retention</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>andor</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>governancerelate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>fraud</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5859</td>\n",
       "      <td>-0.5859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>follow</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>relate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>trendrelate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>climate</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>physical</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>intend</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>tax</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Keyword  Frequency  Sentiment  Weighted Sentiment\n",
       "0            subject         20     0.0000              0.0000\n",
       "1             credit          7     0.3818              2.6726\n",
       "2          associate          5     0.0000              0.0000\n",
       "3      concentration          5     0.0000              0.0000\n",
       "4         additional          4     0.0000              0.0000\n",
       "5        significant          4     0.2023              0.8092\n",
       "6         litigation          3    -0.2023             -0.6069\n",
       "7         regulatory          3     0.0000              0.0000\n",
       "8             expose          3    -0.1531             -0.4593\n",
       "9          liquidity          3     0.0000              0.0000\n",
       "10          exposure          3     0.0000              0.0000\n",
       "11           certain          2     0.2732              0.5464\n",
       "12         unlimited          2     0.0000              0.0000\n",
       "13             great          2     0.6249              1.2498\n",
       "14          numerous          2     0.0000              0.0000\n",
       "15        transition          2     0.0000              0.0000\n",
       "16      counterparty          1     0.0000              0.0000\n",
       "17           factors          1     0.0000              0.0000\n",
       "18            factor          1     0.0000              0.0000\n",
       "19       enforcement          1     0.0000              0.0000\n",
       "20        compliance          1     0.0000              0.0000\n",
       "21           special          1     0.4019              0.4019\n",
       "22        difficulty          1    -0.3400             -0.3400\n",
       "23     climaterelate          1     0.0000              0.0000\n",
       "24      changerelate          1     0.0000              0.0000\n",
       "25         condition          1     0.0000              0.0000\n",
       "26          security          1     0.3400              0.3400\n",
       "27          inherent          1     0.0000              0.0000\n",
       "28          increase          1     0.3182              0.3182\n",
       "29         potential          1     0.0000              0.0000\n",
       "30              pose          1     0.0000              0.0000\n",
       "31        interested          1     0.4019              0.4019\n",
       "32          appetite          1     0.0000              0.0000\n",
       "33        management          1     0.0000              0.0000\n",
       "34              seek          1     0.0000              0.0000\n",
       "35           default          1     0.0000              0.0000\n",
       "36        inaccuracy          1     0.0000              0.0000\n",
       "37             price          1     0.0000              0.0000\n",
       "38              rate          1     0.0000              0.0000\n",
       "39            market          1     0.0000              0.0000\n",
       "40            entail          1     0.0000              0.0000\n",
       "41       foreclosure          1    -0.1280             -0.1280\n",
       "42       delinquency          1     0.0000              0.0000\n",
       "43              type          1     0.0000              0.0000\n",
       "44     macroeconomic          1     0.0000              0.0000\n",
       "45              stem          1     0.0000              0.0000\n",
       "46            single          1     0.0000              0.0000\n",
       "47         retention          1     0.0000              0.0000\n",
       "48             andor          1     0.0000              0.0000\n",
       "49  governancerelate          1     0.0000              0.0000\n",
       "50             fraud          1    -0.5859             -0.5859\n",
       "51            follow          1     0.0000              0.0000\n",
       "52           unknown          1     0.0000              0.0000\n",
       "53            relate          1     0.0000              0.0000\n",
       "54       trendrelate          1     0.0000              0.0000\n",
       "55           climate          1     0.0000              0.0000\n",
       "56          physical          1     0.0000              0.0000\n",
       "57            intend          1     0.0000              0.0000\n",
       "58               tax          1     0.0000              0.0000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze sentiment of the context words\n",
    "Q1_2023context_words_with_sentiment = analyze_word_sentiment(context_words)\n",
    "# Display the context words with sentiment\n",
    "Q1_2023context_words_with_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Investments (where are they investing the money)\n",
    "i. (starting from Page 151 breaking it down from Assets = Liabilities + Equity) \n",
    "\n",
    "    # key word: “investment” -sentences involving investments and frequency \n",
    "    # range: Item 1. Business\n",
    "    # year: 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific Item range of interest\n",
    "df_item = extract_item_specific(\"Item 1. Business\", items_dict=pages_dict, pdf_file=pdf_file)\n",
    "# Preprocess the text(tokenize, lemmatize)\n",
    "df_item_processed = preprocess_text(df_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract context words\n",
    "# Extract context words related to \"risk\"\n",
    "# Count the frequency of each word and display the top 30 words\n",
    "context_words = extract_context_words(df_item_processed, \"investment\", num_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sentiment Score (Including Neutral Words): 0.02577484662576687\n",
      "Weighted Sentiment Score (Excluding Neutral Words): 0.18266521739130437\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Weighted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>strategy</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>opportunity</td>\n",
       "      <td>8</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>3.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portfolio</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adviser</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>management</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>return</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>executing</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>exit</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>pursue</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>underwriting</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Keyword  Frequency  Sentiment  Weighted Sentiment\n",
       "0       strategy          9     0.0000               0.000\n",
       "1    opportunity          8     0.4215               3.372\n",
       "2      portfolio          7     0.0000               0.000\n",
       "3        adviser          6     0.0000               0.000\n",
       "4     management          6     0.0000               0.000\n",
       "..           ...        ...        ...                 ...\n",
       "87        return          1     0.0000               0.000\n",
       "88     executing          1     0.0000               0.000\n",
       "89          exit          1     0.0000               0.000\n",
       "90        pursue          1     0.0000               0.000\n",
       "91  underwriting          1     0.0000               0.000\n",
       "\n",
       "[92 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze sentiment of the context words\n",
    "Q2_2023context_words_with_sentiment = analyze_word_sentiment(context_words)\n",
    "# Display the context words with sentiment\n",
    "Q2_2023context_words_with_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Value Generation( How it is making Money) \n",
    "i. Finding sentiment around the key words of ‘profit’\n",
    "\n",
    "    # key word: \"income\" -sentences involving investments and frequency \n",
    "    # range: Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "    # year: 2023\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific Item range of interestItem 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "df_item = extract_item_specific(\"Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\", items_dict=pages_dict, pdf_file=pdf_file)\n",
    "# Preprocess the text(tokenize, lemmatize)\n",
    "df_item_processed = preprocess_text(df_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract context words\n",
    "# Extract context words related to \"risk\"\n",
    "# Count the frequency of each word and display the top 30 words\n",
    "context_words = extract_context_words(df_item_processed, \"income\", num_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sentiment Score (Including Neutral Words): 0.019273684210526318\n",
      "Weighted Sentiment Score (Excluding Neutral Words): 0.3662\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Weighted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feegenerating</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>total</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>performance</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>base</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeeligible</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Keyword  Frequency  Sentiment  Weighted Sentiment\n",
       "0  feegenerating         15        0.0                 0.0\n",
       "1          total         11        0.0                 0.0\n",
       "2    performance         11        0.0                 0.0\n",
       "3           base          8        0.0                 0.0\n",
       "4    feeeligible          5        0.0                 0.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze sentiment of the context words\n",
    "Q3_2023context_words_with_sentiment = analyze_word_sentiment(context_words)\n",
    "# Display the context words with sentiment\n",
    "Q3_2023context_words_with_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Total AUM change from 2023-2024\n",
    "i. Basic analysis of balance sheet between 2023 and 2024 and AUM delta\n",
    "\n",
    "    # keyword: “AUM”\n",
    "    # Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "    # year 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific Item range of interestItem 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "df_item = extract_item_specific(\"Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\", items_dict=pages_dict, pdf_file=pdf_file)\n",
    "# Preprocess the text(tokenize, lemmatize)\n",
    "df_item_processed = preprocess_text(df_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract context words\n",
    "# Count the frequency of each word and display the top 30 words\n",
    "context_words = extract_context_words(df_item_processed, \"AUM\", num_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sentiment Score (Including Neutral Words): 0.0053117647058823535\n",
      "Weighted Sentiment Score (Excluding Neutral Words): 0.3612\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Weighted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feegenerating</td>\n",
       "      <td>14</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>performance</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>base</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>total</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeeligible</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aum</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nav</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>present</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>apollos</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>end</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>capital</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>asset</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ipo</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>contents</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fee</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>athora</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>subadvised</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>advised</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>service</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>december</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Keyword  Frequency  Sentiment  Weighted Sentiment\n",
       "0   feegenerating         14     0.0000              0.0000\n",
       "1     performance         10     0.0000              0.0000\n",
       "2            base          8     0.0000              0.0000\n",
       "3           total          8     0.0000              0.0000\n",
       "4     feeeligible          5     0.0000              0.0000\n",
       "5             aum          3     0.0000              0.0000\n",
       "6             nav          2     0.0000              0.0000\n",
       "7         present          2     0.0000              0.0000\n",
       "8         apollos          2     0.0000              0.0000\n",
       "9             end          2     0.0000              0.0000\n",
       "10           high          2     0.0000              0.0000\n",
       "11        capital          1     0.0000              0.0000\n",
       "12          asset          1     0.3612              0.3612\n",
       "13            ipo          1     0.0000              0.0000\n",
       "14       contents          1     0.0000              0.0000\n",
       "15            fee          1     0.0000              0.0000\n",
       "16         athora          1     0.0000              0.0000\n",
       "17     subadvised          1     0.0000              0.0000\n",
       "18        advised          1     0.0000              0.0000\n",
       "19        service          1     0.0000              0.0000\n",
       "20       december          1     0.0000              0.0000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze sentiment of the context words\n",
    "Q4_2023context_words_with_sentiment = analyze_word_sentiment(context_words)\n",
    "# Display the context words with sentiment\n",
    "Q4_2023context_words_with_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepocessing for 2024 10-k Starting here\n",
    "# 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Page Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Item 1. Business</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item 1A. Risk Factors</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Item 1B. Unresolved Staff Comments</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Item 1C. Cybersecurity</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Item 2. Properties</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Item 3. Legal Proceedings</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Item 4. Mine Safety Disclosures</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Item 5. Market for Registrant's Common Equity,...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Item 6. [Reserved]</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Item 7. Management's Discussion and Analysis o...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Item 7A. Quantitative and Qualitative Disclosu...</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Item 8. Financial Statements and Supplementary...</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Item 8A. Unaudited Supplemental Presentation o...</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Item 9. Changes in and Disagreements with Acco...</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Item 9A. Controls and Procedures</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Item 9B. Other Information</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Item 9C. Disclosure Regarding Foreign Jurisdic...</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Item 10. Directors, Executive Officers and Cor...</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Item 11. Executive Compensation</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Item 12. Security Ownership of Certain Benefic...</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Item 13. Certain Relationships and Related Tra...</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Item 14. Principal Accountant Fees and Services</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Item 15. Exhibits, Financial Statement Schedules</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Item 16. Form 10-K Summary</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title  Page Number\n",
       "0                                    Item 1. Business           12\n",
       "1                               Item 1A. Risk Factors           31\n",
       "2                  Item 1B. Unresolved Staff Comments           59\n",
       "3                              Item 1C. Cybersecurity           59\n",
       "4                                  Item 2. Properties           62\n",
       "5                           Item 3. Legal Proceedings           62\n",
       "6                     Item 4. Mine Safety Disclosures           62\n",
       "7   Item 5. Market for Registrant's Common Equity,...           63\n",
       "8                                  Item 6. [Reserved]           65\n",
       "9   Item 7. Management's Discussion and Analysis o...           66\n",
       "10  Item 7A. Quantitative and Qualitative Disclosu...          124\n",
       "11  Item 8. Financial Statements and Supplementary...          133\n",
       "12  Item 8A. Unaudited Supplemental Presentation o...          253\n",
       "13  Item 9. Changes in and Disagreements with Acco...          257\n",
       "14                   Item 9A. Controls and Procedures          257\n",
       "15                         Item 9B. Other Information          258\n",
       "16  Item 9C. Disclosure Regarding Foreign Jurisdic...          258\n",
       "17  Item 10. Directors, Executive Officers and Cor...          259\n",
       "18                    Item 11. Executive Compensation          259\n",
       "19  Item 12. Security Ownership of Certain Benefic...          259\n",
       "20  Item 13. Certain Relationships and Related Tra...          259\n",
       "21    Item 14. Principal Accountant Fees and Services          259\n",
       "22   Item 15. Exhibits, Financial Statement Schedules          260\n",
       "23                         Item 16. Form 10-K Summary          274"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the pdf\n",
    "pdf_file = \"Apollo-24-10-k.pdf\"  # Replace with actual file path\n",
    "\n",
    "# Items pages dataframe (input the page number mannuly)\n",
    "pages_df = item_pages_df(pdf_path=pdf_file, page_num=2)[0]\n",
    "pages_dict = item_pages_df(pdf_path=pdf_file, page_num=2)[1]\n",
    "\n",
    "pages_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1.How it has changed from 2023-2024 (Risk Factors more or less and to what degree) \n",
    "i. Risk Profiling by segmenting key words from 1A from both 2023 and 2024\n",
    "\n",
    "    # key word: “risk”\n",
    "    # range: Item 1A. Risk Factors\n",
    "    # year: 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific Item range of interest\n",
    "df_item = extract_item_specific(\"Item 1A. Risk Factors\", items_dict=pages_dict, pdf_file=pdf_file)\n",
    "# Preprocess the text(tokenize, lemmatize)\n",
    "df_item_processed = preprocess_text(df_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract context words\n",
    "# Extract context words related to \"risk\"\n",
    "# Count the frequency of each word and display the top 30 words\n",
    "context_words = extract_context_words(df_item_processed, \"risk\", num_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sentiment Score (Including Neutral Words): 0.019273684210526318\n",
      "Weighted Sentiment Score (Excluding Neutral Words): 0.3662\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Weighted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feegenerating</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>total</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>performance</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>base</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feeeligible</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Keyword  Frequency  Sentiment  Weighted Sentiment\n",
       "0  feegenerating         15        0.0                 0.0\n",
       "1          total         11        0.0                 0.0\n",
       "2    performance         11        0.0                 0.0\n",
       "3           base          8        0.0                 0.0\n",
       "4    feeeligible          5        0.0                 0.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze sentiment of the context words\n",
    "Q1_2024context_words_with_sentiment = analyze_word_sentiment(context_words)\n",
    "# Display the context words with sentiment\n",
    "Q1_2024context_words_with_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. Investments (where are they investing the money)\n",
    "i. (starting from Page 151 breaking it down from Assets = Liabilities + Equity) \n",
    "\n",
    "    # key word: “investment” -sentences involving investments and frequency \n",
    "    # range: Item 1. Business\n",
    "    # year: 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific Item range of interest\n",
    "df_item = extract_item_specific(\"Item 1. Business\", items_dict=pages_dict, pdf_file=pdf_file)\n",
    "# Preprocess the text(tokenize, lemmatize)\n",
    "df_item_processed = preprocess_text(df_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract context words\n",
    "# Extract context words related to \"risk\"\n",
    "# Count the frequency of each word and display the top 30 words\n",
    "context_words = extract_context_words(df_item_processed, \"investment\", num_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sentiment Score (Including Neutral Words): 0.0072130081300813016\n",
      "Weighted Sentiment Score (Excluding Neutral Words): 0.14786666666666667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Weighted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>income</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fund</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portfolio</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>net</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>total</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Keyword  Frequency  Sentiment  Weighted Sentiment\n",
       "0     income         37        0.0                 0.0\n",
       "1       fund         23        0.0                 0.0\n",
       "2  portfolio         21        0.0                 0.0\n",
       "3        net         17        0.0                 0.0\n",
       "4      total         17        0.0                 0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze sentiment of the context words\n",
    "Q2_2024context_words_with_sentiment = analyze_word_sentiment(context_words)\n",
    "# Display the context words with sentiment\n",
    "Q2_2024context_words_with_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. Value Generation( How it is making Money) \n",
    "i. Finding sentiment around the key words of ‘profit’\n",
    "\n",
    "    # key word: \"income\" -sentences involving investments and frequency \n",
    "    # range: Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "    # year: 2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific Item range of interestItem 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "df_item = extract_item_specific(\"Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\", items_dict=pages_dict, pdf_file=pdf_file)\n",
    "# Preprocess the text(tokenize, lemmatize)\n",
    "df_item_processed = preprocess_text(df_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract context words\n",
    "# Extract context words related to \"risk\"\n",
    "# Count the frequency of each word and display the top 30 words\n",
    "context_words = extract_context_words(df_item_processed, \"income\", num_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sentiment Score (Including Neutral Words): 0.0072130081300813016\n",
      "Weighted Sentiment Score (Excluding Neutral Words): 0.14786666666666667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Weighted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>income</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fund</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portfolio</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>net</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>total</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Keyword  Frequency  Sentiment  Weighted Sentiment\n",
       "0     income         37        0.0                 0.0\n",
       "1       fund         23        0.0                 0.0\n",
       "2  portfolio         21        0.0                 0.0\n",
       "3        net         17        0.0                 0.0\n",
       "4      total         17        0.0                 0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze sentiment of the context words\n",
    "Q3_2024context_words_with_sentiment = analyze_word_sentiment(context_words)\n",
    "# Display the context words with sentiment\n",
    "Q3_2024context_words_with_sentiment.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4. Total AUM change from 2023 - 2024\n",
    "i. Basic analysis of balance sheet between 2023 and 2024 and AUM delta\n",
    "\n",
    "    # keyword: “AUM”\n",
    "    # Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "    # year 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific Item range of interestItem 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\n",
    "df_item = extract_item_specific(\"Item 7. Management's Discussion and Analysis of Financial Condition and Results of Operations\", items_dict=pages_dict, pdf_file=pdf_file)\n",
    "# Preprocess the text(tokenize, lemmatize)\n",
    "df_item_processed = preprocess_text(df_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract context words\n",
    "# Count the frequency of each word and display the top 30 words\n",
    "context_words = extract_context_words(df_item_processed, \"AUM\", num_display=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Sentiment Score (Including Neutral Words): 0.0072130081300813016\n",
      "Weighted Sentiment Score (Excluding Neutral Words): 0.14786666666666667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Weighted Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>income</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fund</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>portfolio</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>net</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>total</td>\n",
       "      <td>17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Keyword  Frequency  Sentiment  Weighted Sentiment\n",
       "0     income         37        0.0                 0.0\n",
       "1       fund         23        0.0                 0.0\n",
       "2  portfolio         21        0.0                 0.0\n",
       "3        net         17        0.0                 0.0\n",
       "4      total         17        0.0                 0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze sentiment of the context words\n",
    "Q4_2024context_words_with_sentiment = analyze_word_sentiment(context_words)\n",
    "# Display the context words with sentiment\n",
    "Q4_2024context_words_with_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words for Q1 2023:\n",
      "         Keyword  Frequency\n",
      "0        subject         20\n",
      "1         credit          7\n",
      "2      associate          5\n",
      "3  concentration          5\n",
      "4     additional          4\n",
      "\n",
      "Top 5 words for Q1 2024:\n",
      "         Keyword  Frequency\n",
      "0  feegenerating         15\n",
      "2    performance         11\n",
      "1          total         11\n",
      "3           base          8\n",
      "4    feeeligible          5\n",
      "\n",
      "Top 5 words for Q2 2023:\n",
      "       Keyword  Frequency\n",
      "0     strategy          9\n",
      "1  opportunity          8\n",
      "2    portfolio          7\n",
      "3      adviser          6\n",
      "4   management          6\n",
      "\n",
      "Top 5 words for Q2 2024:\n",
      "     Keyword  Frequency\n",
      "0     income         37\n",
      "1       fund         23\n",
      "2  portfolio         21\n",
      "3        net         17\n",
      "4      total         17\n",
      "\n",
      "Top 5 words for Q3 2023:\n",
      "         Keyword  Frequency\n",
      "0  feegenerating         15\n",
      "2    performance         11\n",
      "1          total         11\n",
      "3           base          8\n",
      "4    feeeligible          5\n",
      "\n",
      "Top 5 words for Q3 2024:\n",
      "     Keyword  Frequency\n",
      "0     income         37\n",
      "1       fund         23\n",
      "2  portfolio         21\n",
      "3        net         17\n",
      "4      total         17\n",
      "\n",
      "Top 5 words for Q4 2023:\n",
      "         Keyword  Frequency\n",
      "0  feegenerating         14\n",
      "1    performance         10\n",
      "2           base          8\n",
      "3          total          8\n",
      "4    feeeligible          5\n",
      "\n",
      "Top 5 words for Q4 2024:\n",
      "         Keyword  Frequency\n",
      "0  feegenerating         15\n",
      "2    performance         11\n",
      "1          total         11\n",
      "3           base          8\n",
      "4    feeeligible          5\n"
     ]
    }
   ],
   "source": [
    "def get_top_10_words(df_2023, df_2024):\n",
    "    # Sort the DataFrames by 'Frequency' in descending order\n",
    "    top_10_2023 = df_2023.sort_values(by='Frequency', ascending=False).head(10)\n",
    "    top_10_2024 = df_2024.sort_values(by='Frequency', ascending=False).head(10)\n",
    "    \n",
    "    return top_10_2023, top_10_2024\n",
    "\n",
    "# Get the top 10 words for Q1\n",
    "top_10_Q1_2023, top_10_Q1_2024 = get_top_10_words(Q1_2023context_words_with_sentiment, Q1_2024context_words_with_sentiment)\n",
    "print(\"Top 5 words for Q1 2023:\")\n",
    "print(top_10_Q1_2023[['Keyword', 'Frequency']].head())\n",
    "print(\"\\nTop 5 words for Q1 2024:\")\n",
    "print(top_10_Q1_2024[['Keyword', 'Frequency']].head())\n",
    "\n",
    "# Get the top 10 words for Q2\n",
    "top_10_Q2_2023, top_10_Q2_2024 = get_top_10_words(Q2_2023context_words_with_sentiment, Q2_2024context_words_with_sentiment)\n",
    "print(\"\\nTop 5 words for Q2 2023:\")\n",
    "print(top_10_Q2_2023[['Keyword', 'Frequency']].head())\n",
    "print(\"\\nTop 5 words for Q2 2024:\")\n",
    "print(top_10_Q2_2024[['Keyword', 'Frequency']].head())\n",
    "\n",
    "# Get the top 10 words for Q3\n",
    "top_10_Q3_2023, top_10_Q3_2024 = get_top_10_words(Q3_2023context_words_with_sentiment, Q3_2024context_words_with_sentiment)\n",
    "print(\"\\nTop 5 words for Q3 2023:\")\n",
    "print(top_10_Q3_2023[['Keyword', 'Frequency']].head())\n",
    "print(\"\\nTop 5 words for Q3 2024:\")\n",
    "print(top_10_Q3_2024[['Keyword', 'Frequency']].head())\n",
    "\n",
    "# Get the top 10 words for Q4\n",
    "top_10_Q4_2023, top_10_Q4_2024 = get_top_10_words(Q4_2023context_words_with_sentiment, Q4_2024context_words_with_sentiment)\n",
    "print(\"\\nTop 5 words for Q4 2023:\")\n",
    "print(top_10_Q4_2023[['Keyword', 'Frequency']].head())\n",
    "print(\"\\nTop 5 words for Q4 2024:\")\n",
    "print(top_10_Q4_2024[['Keyword', 'Frequency']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Sentiment Comparison:\n",
      "  Keyword  Weighted Sentiment_2023  Weighted Sentiment_2024  \\\n",
      "0  credit                   2.6726                   0.7636   \n",
      "\n",
      "   Sentiment_Difference  \n",
      "0                -1.909  \n",
      "\n",
      "Q2 Sentiment Comparison:\n",
      "       Keyword  Weighted Sentiment_2023  Weighted Sentiment_2024  \\\n",
      "0     strategy                    0.000                   0.0000   \n",
      "1  opportunity                    3.372                   1.2645   \n",
      "2    portfolio                    0.000                   0.0000   \n",
      "3   management                    0.000                   0.0000   \n",
      "4         fund                    0.000                   0.0000   \n",
      "\n",
      "   Sentiment_Difference  \n",
      "0                0.0000  \n",
      "1               -2.1075  \n",
      "2                0.0000  \n",
      "3                0.0000  \n",
      "4                0.0000  \n",
      "\n",
      "Q3 Sentiment Comparison:\n",
      "       Keyword  Weighted Sentiment_2023  Weighted Sentiment_2024  \\\n",
      "0        total                      0.0                      0.0   \n",
      "1  performance                      0.0                      0.0   \n",
      "2         base                      0.0                      0.0   \n",
      "3       equity                      0.0                      0.0   \n",
      "4      present                      0.0                      0.0   \n",
      "\n",
      "   Sentiment_Difference  \n",
      "0                   0.0  \n",
      "1                   0.0  \n",
      "2                   0.0  \n",
      "3                   0.0  \n",
      "4                   0.0  \n",
      "\n",
      "Q4 Sentiment Comparison:\n",
      "         Keyword  Weighted Sentiment_2023  Weighted Sentiment_2024  \\\n",
      "0  feegenerating                      0.0                      0.0   \n",
      "1    performance                      0.0                      0.0   \n",
      "2           base                      0.0                      0.0   \n",
      "3          total                      0.0                      0.0   \n",
      "4    feeeligible                      0.0                      0.0   \n",
      "\n",
      "   Sentiment_Difference  \n",
      "0                   0.0  \n",
      "1                   0.0  \n",
      "2                   0.0  \n",
      "3                   0.0  \n",
      "4                   0.0  \n",
      "Sentiment Comparison for Q1:\n",
      "{'Sum Weighted Sentiment 2023': 4.6198999999999995, 'Sum Weighted Sentiment 2024': 1.4648, 'Difference': -3.155099999999999}\n"
     ]
    }
   ],
   "source": [
    "def compare_sentiment(df_2023, df_2024):\n",
    "    # Merge the two dataframes on the 'Keyword' column\n",
    "    comparison_df = pd.merge(df_2023[['Keyword', 'Weighted Sentiment']], \n",
    "                             df_2024[['Keyword', 'Weighted Sentiment']], \n",
    "                             on='Keyword', \n",
    "                             suffixes=('_2023', '_2024'))\n",
    "    \n",
    "    # Calculate the sentiment difference\n",
    "    comparison_df['Sentiment_Difference'] = comparison_df['Weighted Sentiment_2024'] - comparison_df['Weighted Sentiment_2023']\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare sentiment for Q1\n",
    "Q1_sentiment_comparison = compare_sentiment(Q1_2023context_words_with_sentiment, Q1_2024context_words_with_sentiment)\n",
    "print(\"Q1 Sentiment Comparison:\")\n",
    "print(Q1_sentiment_comparison.head())\n",
    "\n",
    "# Compare sentiment for Q2\n",
    "Q2_sentiment_comparison = compare_sentiment(Q2_2023context_words_with_sentiment, Q2_2024context_words_with_sentiment)\n",
    "print(\"\\nQ2 Sentiment Comparison:\")\n",
    "print(Q2_sentiment_comparison.head())\n",
    "\n",
    "# Compare sentiment for Q3\n",
    "Q3_sentiment_comparison = compare_sentiment(Q3_2023context_words_with_sentiment, Q3_2024context_words_with_sentiment)\n",
    "print(\"\\nQ3 Sentiment Comparison:\")\n",
    "print(Q3_sentiment_comparison.head())\n",
    "\n",
    "# Compare sentiment for Q4\n",
    "Q4_sentiment_comparison = compare_sentiment(Q4_2023context_words_with_sentiment, Q4_2024context_words_with_sentiment)\n",
    "print(\"\\nQ4 Sentiment Comparison:\")\n",
    "print(Q4_sentiment_comparison.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Comparison for Q1 for Risk Factors:\n",
      "{'Sum Weighted Sentiment 2023': 4.62, 'Sum Weighted Sentiment 2024': 1.46, 'Difference': -3.16}\n",
      "Sentiment Comparison for Q2 For Investments:\n",
      "{'Sum Weighted Sentiment Q2 2023': 4.2, 'Sum Weighted Sentiment Q2 2024': 2.66, 'Difference': -1.54}\n",
      "Sentiment Comparison for Q3 For Profit:\n",
      "{'Sum Weighted Sentiment Q3 2023': 1.46, 'Sum Weighted Sentiment Q3 2024': 2.66, 'Difference': 1.2}\n",
      "Sentiment Comparison for Q4 For change in AUM:\n",
      "{'Sum Weighted Sentiment Q4 2023': 0.36, 'Sum Weighted Sentiment Q4 2024': 1.46, 'Difference': 1.1}\n"
     ]
    }
   ],
   "source": [
    "# Compute the sum of the weighted sentiment for 2023 and 2024\n",
    "sum_weighted_sentiment_2023 = round(Q1_2023context_words_with_sentiment['Weighted Sentiment'].sum(), 2)\n",
    "sum_weighted_sentiment_2024 = round(Q1_2024context_words_with_sentiment['Weighted Sentiment'].sum(), 2)\n",
    "\n",
    "# Compare the two sums\n",
    "sentiment_comparison = {\n",
    "    \"Sum Weighted Sentiment 2023\": sum_weighted_sentiment_2023,\n",
    "    \"Sum Weighted Sentiment 2024\": sum_weighted_sentiment_2024,\n",
    "    \"Difference\": round(sum_weighted_sentiment_2024 - sum_weighted_sentiment_2023, 2)\n",
    "}\n",
    "\n",
    "print(\"Sentiment Comparison for Q1 for Risk Factors:\")\n",
    "print(sentiment_comparison)\n",
    "\n",
    "# Compute the sum of the weighted sentiment for Q2 2023 and Q2 2024\n",
    "sum_weighted_sentiment_Q2_2023 = round(Q2_2023context_words_with_sentiment['Weighted Sentiment'].sum(), 2)\n",
    "sum_weighted_sentiment_Q2_2024 = round(Q2_2024context_words_with_sentiment['Weighted Sentiment'].sum(), 2)\n",
    "\n",
    "# Compare the two sums for Q2\n",
    "sentiment_comparison_Q2 = {\n",
    "    \"Sum Weighted Sentiment Q2 2023\": sum_weighted_sentiment_Q2_2023,\n",
    "    \"Sum Weighted Sentiment Q2 2024\": sum_weighted_sentiment_Q2_2024,\n",
    "    \"Difference\": round(sum_weighted_sentiment_Q2_2024 - sum_weighted_sentiment_Q2_2023, 2)\n",
    "}\n",
    "\n",
    "print(\"Sentiment Comparison for Q2 For Investments:\")\n",
    "print(sentiment_comparison_Q2)\n",
    "\n",
    "# Compute the sum of the weighted sentiment for Q3 2023 and Q3 2024\n",
    "sum_weighted_sentiment_Q3_2023 = round(Q3_2023context_words_with_sentiment['Weighted Sentiment'].sum(), 2)\n",
    "sum_weighted_sentiment_Q3_2024 = round(Q3_2024context_words_with_sentiment['Weighted Sentiment'].sum(), 2)\n",
    "\n",
    "# Compare the two sums for Q3\n",
    "sentiment_comparison_Q3 = {\n",
    "    \"Sum Weighted Sentiment Q3 2023\": sum_weighted_sentiment_Q3_2023,\n",
    "    \"Sum Weighted Sentiment Q3 2024\": sum_weighted_sentiment_Q3_2024,\n",
    "    \"Difference\": round(sum_weighted_sentiment_Q3_2024 - sum_weighted_sentiment_Q3_2023, 2)\n",
    "}\n",
    "\n",
    "print(\"Sentiment Comparison for Q3 For Profit:\")\n",
    "print(sentiment_comparison_Q3)\n",
    "\n",
    "# Compute the sum of the weighted sentiment for Q4 2023 and Q4 2024\n",
    "sum_weighted_sentiment_Q4_2023 = round(Q4_2023context_words_with_sentiment['Weighted Sentiment'].sum(), 2)\n",
    "sum_weighted_sentiment_Q4_2024 = round(Q4_2024context_words_with_sentiment['Weighted Sentiment'].sum(), 2)\n",
    "\n",
    "# Compare the two sums for Q4\n",
    "sentiment_comparison_Q4 = {\n",
    "    \"Sum Weighted Sentiment Q4 2023\": sum_weighted_sentiment_Q4_2023,\n",
    "    \"Sum Weighted Sentiment Q4 2024\": sum_weighted_sentiment_Q4_2024,\n",
    "    \"Difference\": round(sum_weighted_sentiment_Q4_2024 - sum_weighted_sentiment_Q4_2023, 2)\n",
    "}\n",
    "\n",
    "print(\"Sentiment Comparison for Q4 For change in AUM:\")\n",
    "print(sentiment_comparison_Q4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
